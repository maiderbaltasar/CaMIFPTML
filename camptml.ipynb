{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":9526201,"sourceType":"datasetVersion","datasetId":5800885},{"sourceId":10715208,"sourceType":"datasetVersion","datasetId":6641723},{"sourceId":10715472,"sourceType":"datasetVersion","datasetId":6641903}],"dockerImageVersionId":30886,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport numpy as np\nimport os\nimport pandas as pd\nimport seaborn as sns\nimport xgboost as xgb\nfrom sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier\nfrom sklearn.feature_selection import VarianceThreshold\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import (accuracy_score, classification_report, confusion_matrix, ConfusionMatrixDisplay, f1_score, roc_auc_score, roc_curve)\nfrom sklearn.model_selection import GridSearchCV, RandomizedSearchCV, train_test_split\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.metrics import roc_auc_score, roc_curve\nfrom sklearn.metrics import roc_curve, auc\nfrom sklearn.preprocessing import MinMaxScaler \nfrom sklearn.model_selection import cross_validate\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import StratifiedKFold","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Function to plot confusion matrix with percentages\ndef plot_confusion_matrix_with_percentages(y_true, y_pred, title=\"Confusion Matrix\", filename=\"confusion_matrix.png\"):\n    cm = confusion_matrix(y_true, y_pred)\n    cm_percentage = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis] * 100  # Percentages of each class\n    labels = np.unique(y_true)  # Assumes that the unique labels are the same for both sets\n\n    # Create a heatmap\n    plt.figure(figsize=(6, 4))\n    sns.heatmap(cm_percentage, annot=True, fmt='.2f', cmap='Blues', xticklabels=labels, yticklabels=labels)\n    plt.title(title)\n    plt.xlabel('Predicted')\n    plt.ylabel('True')\n    plt.tight_layout()\n    \n    # Save the plot\n    plt.savefig(filename)\n    plt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Define the function for plotting feature importances without numbers\ndef plot_feature_importance(features_df, title, filename):\n    # Normalize the importance values to scale them between 0 and 1 for the gradient\n    scaler = MinMaxScaler()\n    features_df['Importance_Scaled'] = scaler.fit_transform(features_df[['Importance']])\n\n    # Plot the top 20 features\n    plt.figure(figsize=(10, 6))  # Adjust size as needed\n    sns.barplot(x='Importance', y='Feature', data=features_df, palette='viridis')\n\n    # Customize the plot\n    plt.title(title, fontsize=18)\n    plt.xlabel('Importance', fontsize=14)\n    plt.ylabel('Feature', fontsize=14)\n    plt.xticks(fontsize=14)\n    plt.yticks(fontsize=14)\n    \n    # Save the plot\n    plt.tight_layout()\n    plt.savefig(filename, dpi=300)  # Save the plot with high resolution\n\n    # Show the plot\n    plt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"for dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Load the dataset\nexcel_file = '/kaggle/input/paper03data/Python.xlsx'\ndf = pd.read_excel(excel_file)\n\n# Preview the data\nprint(df.head())","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Select most relevant features","metadata":{}},{"cell_type":"code","source":"# Compute the correlation matrix\ncorrelation_matrix = df.corr()\n\n# Visualize the correlation matrix\nplt.figure(figsize=(12, 8))\nsns.heatmap(correlation_matrix, annot=False, cmap='coolwarm')\nplt.title(\"Feature Correlation Matrix\", fontsize=16)\n\n# Save the figure in high quality\nsave_path = '/kaggle/working/correlation_matrix.png'\nplt.savefig(save_path, dpi=300, bbox_inches='tight') \n\n# Show the figure\nplt.show()\n\nprint(f\"Correlation matrix figure saved at: {save_path}\")\n\n# Set a correlation threshold\nthreshold = 0.97\n\n# Identify features with high correlation\ncorrelated_features = set()\nfor i in range(len(correlation_matrix.columns)):\n    for j in range(i):\n        if abs(correlation_matrix.iloc[i, j]) > threshold:\n            correlated_features.add(correlation_matrix.columns[i])\n\nprint(f\"Features to be removed due to high correlation: {correlated_features}\")\n\n# Drop highly correlated features\ndf = df.drop(columns=correlated_features)\n\nprint(f\"Remaining columns after correlation filter: {df.columns}\")\n\n# Preview the data\nprint(df.head())","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Define the variance threshold\nvar_threshold = 0.01\n\n# Apply variance thresholding\nselector = VarianceThreshold(threshold=var_threshold)\ndf_high_variance = selector.fit_transform(df)\n\n# Get the remaining features\nselected_features = df.columns[selector.get_support()]\ndf = pd.DataFrame(df_high_variance, columns=selected_features)\n\nprint(f\"Remaining columns after variance filter: {df.columns}\")\n\n# Preview the data\nprint(df.head())","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Define the model","metadata":{}},{"cell_type":"code","source":"# Define the features and the target\nX = df.drop(columns=['f(vij)obj'])\ny = df['f(vij)obj']\n\n# Split the data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Run all models","metadata":{}},{"cell_type":"code","source":"# Define the number of folds\nkfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Define models\nmodels = {\n    'Random Forest': RandomForestClassifier(),\n    'SVM': SVC(probability=True),\n    'Decision Tree': DecisionTreeClassifier(),\n    'KNN': KNeighborsClassifier(),\n    'Gradient Boosting': GradientBoostingClassifier(),\n    'XGBoost': xgb.XGBClassifier(use_label_encoder=False, eval_metric='logloss'),\n}","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Dictionary to store cross-validation results\nmodel_results = {}\n\n# Evaluate models using K-Fold Cross Validation\nfor model_name, model in models.items():\n    print(f'Evaluating {model_name} with K-Fold...')\n    \n    accuracy_scores = []\n    roc_auc_scores = []\n\n    for train_idx, test_idx in kfold.split(X, y):\n        X_train_fold, X_test_fold = X.iloc[train_idx], X.iloc[test_idx]\n        y_train_fold, y_test_fold = y.iloc[train_idx], y.iloc[test_idx]\n        \n        # Train the model\n        model.fit(X_train_fold, y_train_fold)\n        \n        # Make predictions\n        y_pred = model.predict(X_test_fold)\n        y_proba = model.predict_proba(X_test_fold)[:, 1] if hasattr(model, \"predict_proba\") else None\n        \n        # Compute metrics\n        acc = accuracy_score(y_test_fold, y_pred)\n        roc_auc = roc_auc_score(y_test_fold, y_proba) if y_proba is not None else None\n        \n        accuracy_scores.append(acc)\n        if roc_auc is not None:\n            roc_auc_scores.append(roc_auc)\n    \n    # Store the mean metrics\n    model_results[model_name] = {\n        'Mean Accuracy': np.mean(accuracy_scores),\n        'Mean ROC AUC': np.mean(roc_auc_scores) if roc_auc_scores else None\n    }\n\n    print(f'{model_name} - Accuracy: {np.mean(accuracy_scores):.2f}, ROC AUC: {np.mean(roc_auc_scores) if roc_auc_scores else \"N/A\"}')\n\n# Convert results to a DataFrame\nresults_df = pd.DataFrame.from_dict(model_results, orient='index')\nprint(results_df)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Exclude LDA from plots\nnon_linear_models = {name: model for name, model in models.items() if name != 'LDA'}\n\n# Extract model names, accuracy, and ROC AUC scores\nmodel_names = list(non_linear_models.keys())\ntrain_accuracies = [model_results[name]['Mean Accuracy'] for name in model_names]\nroc_aucs = [model_results[name]['Mean ROC AUC'] for name in model_names]\n\n# HORIZONTAL BAR PLOT - TRAINING ACCURACY & TEST ACCURACY\nplt.figure(figsize=(10, 6))\ncolors = sns.color_palette(\"viridis\", len(model_names))\n\nsns.barplot(x=train_accuracies, y=model_names, palette=colors)\n\nplt.xlabel('Mean Accuracy')\nplt.ylabel('Model')\nplt.title('Training & Test Accuracy for Non-Linear Models')\nplt.xlim(0, 1)\n\n# Save the figure\nplt.savefig(\"model_accuracy_barplot.png\", dpi=300, bbox_inches=\"tight\")\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ROC Curves for Non-Linear Models\nnon_linear_models = {name: model for name, model in models.items() if name not in ['LDA', 'Logistic Regression']}\n\nplt.figure(figsize=(10, 6))\nfor model_name, model in non_linear_models.items():\n    model.fit(X_train, y_train)\n    y_train_proba = model.predict_proba(X_train)[:, 1]\n    y_test_proba = model.predict_proba(X_test)[:, 1]\n\n    # Compute ROC curves\n    fpr_train, tpr_train, _ = roc_curve(y_train, y_train_proba)\n    fpr_test, tpr_test, _ = roc_curve(y_test, y_test_proba)\n\n    # Plot\n    plt.plot(fpr_train, tpr_train, linestyle='--', label=f'{model_name} - Train')\n    plt.plot(fpr_test, tpr_test, label=f'{model_name} - Test')\n\n# Plot Random Classifier line\nplt.plot([0, 1], [0, 1], linestyle='dotted', color='black')\n\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('ROC Curves for Non-Linear Models')\nplt.legend()\n\n# Save the ROC curve plot\nplt.savefig(\"roc_curves_non_linear_models.png\", dpi=300, bbox_inches='tight')\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Select the best model based on highest mean accuracy from cross-validation\nbest_model_name = max(model_results, key=lambda k: model_results[k]['Mean Accuracy'])\nbest_model = models[best_model_name]\n\nprint(f\"\\nRetraining the best model: {best_model_name} on full training data...\")\n\n# Train the best model on the full training set\nbest_model.fit(X_train, y_train)\n\n# Make predictions on the test set\ny_test_pred = best_model.predict(X_test)\ny_test_proba = best_model.predict_proba(X_test)[:, 1] if hasattr(best_model, \"predict_proba\") else None\n\n# Compute final performance metrics\nfinal_accuracy = accuracy_score(y_test, y_test_pred)\nfinal_roc_auc = roc_auc_score(y_test, y_test_proba) if y_test_proba is not None else None\n\n# Display final results\nprint(f\"\\nFinal Test Set Results for {best_model_name}:\")\nprint(f\"Accuracy: {final_accuracy:.2f}\")\nprint(f\"ROC AUC: {final_roc_auc:.2f}\" if final_roc_auc is not None else \"ROC AUC: N/A\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 1. LDA","metadata":{}},{"cell_type":"code","source":"# Initialize the LDA model\nlda_model = LinearDiscriminantAnalysis()\n\n# Train the model\nlda_model.fit(X_train, y_train)\n\n# Predictions\ny_train_pred_lda = lda_model.predict(X_train)\ny_test_pred_lda = lda_model.predict(X_test)\n\n# Accuracy\ntrain_accuracy_lda = accuracy_score(y_train, y_train_pred_lda)\ntest_accuracy_lda = accuracy_score(y_test, y_test_pred_lda)\n\nprint(f'LDA - Training Accuracy: {train_accuracy_lda:.2f}')\nprint(f'LDA - Test Accuracy: {test_accuracy_lda:.2f}')\nprint(\"Classification Report for LDA (Training Set):\")\nprint(classification_report(y_train, y_train_pred_lda))\nprint(\"Classification Report for LDA (Test Set):\")\nprint(classification_report(y_test, y_test_pred_lda))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Add Randomized SearchCV**","metadata":{}},{"cell_type":"code","source":"# Define the parameter grid for RandomizedSearchCV\nparam_dist_lda = {\n    'solver': ['svd', 'lsqr', 'eigen'],  # We will try all three solvers\n    'shrinkage': [None, 0.0, 0.1, 0.2, 0.3, 0.5, 0.7, 1.0]  # Shrinkage for lsqr and eigen only\n}\n\n# Adjusting RandomizedSearchCV parameters to handle the shrinkage issue\nrandom_search_lda = RandomizedSearchCV(\n    estimator=lda_model,\n    param_distributions=param_dist_lda,\n    n_iter=15,  # Reduce iterations to 15 to avoid excessive computation\n    cv=3,  # Cross-validation folds\n    verbose=2,\n    random_state=7,\n    n_jobs=-1\n)\n\n# Fit the RandomizedSearchCV on the training data\nrandom_search_lda.fit(X_train, y_train)\n\n# Get the best parameters from RandomizedSearchCV\nbest_params_random_lda = random_search_lda.best_params_\nprint(f\"Best parameters from Randomized Search for LDA: {best_params_random_lda}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Add GridSearchCV**","metadata":{}},{"cell_type":"code","source":"# Ensure valid shrinkage values for LDA\ndef get_valid_shrinkage(solver, shrinkage):\n    if solver == 'svd':\n        return [None]  # 'svd' does not support shrinkage\n    elif solver in ['lsqr', 'eigen'] and shrinkage is not None:\n        # For lsqr and eigen solvers, generate valid shrinkage values\n        return [max(0.0, shrinkage - 0.1), shrinkage, min(1.0, shrinkage + 0.1)]\n    return [None]  # Return None by default for unsupported solvers or when shrinkage is None\n\n# Extract the best solver and shrinkage from RandomizedSearchCV\nbest_solver_lda = best_params_random_lda['solver']\nbest_shrinkage_lda = best_params_random_lda.get('shrinkage', None)\n\n# Create the parameter grid for GridSearchCV based on RandomizedSearchCV result\nparam_grid_lda = {\n    'solver': [best_solver_lda],\n    'shrinkage': get_valid_shrinkage(best_solver_lda, best_shrinkage_lda)\n}\n\n# Set up GridSearchCV for LDA\ngrid_search_lda = GridSearchCV(\n    estimator=lda_model,\n    param_grid=param_grid_lda,\n    cv=3,\n    verbose=2,\n    n_jobs=-1\n)\n\n# Fit GridSearchCV on the training data\ngrid_search_lda.fit(X_train, y_train)\n\n# Get the best parameters from GridSearchCV\nbest_params_grid_lda = grid_search_lda.best_params_\nprint(f\"Best parameters from Grid Search for LDA: {best_params_grid_lda}\")\n\n# Predict using the best model from GridSearchCV\nbest_lda_model = grid_search_lda.best_estimator_\ny_test_pred_lda = best_lda_model.predict(X_test)\n\n# Evaluate the model performance\ntest_accuracy_lda = accuracy_score(y_test, y_test_pred_lda)\nprint(f'Test Set Accuracy with Best Hyperparameters from Grid Search: {test_accuracy_lda * 100:.2f}%')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Evaluate the model**","metadata":{}},{"cell_type":"code","source":"# Get the best model from Grid Search\nbest_lda_model = grid_search_lda.best_estimator_\n\n# Predictions on the training set\ny_train_pred_lda = best_lda_model.predict(X_train)\n\n# Predictions on the test set\ny_test_pred_lda = best_lda_model.predict(X_test)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 1. Accuracy Score for the Training Set\ntrain_accuracy_lda = accuracy_score(y_train, y_train_pred_lda)\nprint(f'Train Set Accuracy (LDA): {train_accuracy_lda * 100:.2f}%')\n\n# 2. Accuracy Score for the Test Set\ntest_accuracy_lda = accuracy_score(y_test, y_test_pred_lda)\nprint(f'Test Set Accuracy (LDA): {test_accuracy_lda * 100:.2f}%')\n\n# How many predictions were correct out of the total for both sets\ntrain_correct_lda = sum(y_train_pred_lda == y_train)\ntest_correct_lda = sum(y_test_pred_lda == y_test)\n\nprint(f'Training set (LDA): {train_correct_lda} correct out of {len(y_train)}')\nprint(f'Test set (LDA): {test_correct_lda} correct out of {len(y_test)}')\n\n# Sensitivity (Recall) and Specificity for Training Set\ncm_train_lda = confusion_matrix(y_train, y_train_pred_lda)\nif cm_train_lda.shape == (2, 2):\n    TN, FP, FN, TP = cm_train_lda.ravel()\n    sensitivity_train_lda = TP / (TP + FN)\n    specificity_train_lda = TN / (TN + FP)\nelse:\n    sensitivity_train_lda = recall_score(y_train, y_train_pred_lda, average='macro')\n    specificity_train_lda = np.nan  # Not defined for multiclass\n\n# Sensitivity (Recall) and Specificity for Test Set\ncm_test_lda = confusion_matrix(y_test, y_test_pred_lda)\nif cm_test_lda.shape == (2, 2):\n    TN, FP, FN, TP = cm_test_lda.ravel()\n    sensitivity_test_lda = TP / (TP + FN)\n    specificity_test_lda = TN / (TN + FP)\nelse:\n    sensitivity_test_lda = recall_score(y_test, y_test_pred_lda, average='macro')\n    specificity_test_lda = np.nan  # Not defined for multiclass\n\n# 3. Summary Table\nsummary_lda = pd.DataFrame({\n    'Metric': ['Accuracy', 'Sensitivity', 'Specificity'],\n    'Training Set': [train_accuracy_lda, sensitivity_train_lda, specificity_train_lda],\n    'Test Set': [test_accuracy_lda, sensitivity_test_lda, specificity_test_lda]\n})\n\nprint(\"\\nLDA Performance Summary:\")\nprint(summary_lda)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Initialize variables\nif cm_train_lda.shape == (2, 2):  # Binary classification\n    TN_train, FP_train, FN_train, TP_train = cm_train_lda.ravel()\n    TN_test, FP_test, FN_test, TP_test = cm_test_lda.ravel()\n    \n    # Sensitivity (Recall) & Specificity calculations\n    sensitivity_train_lda = TP_train / (TP_train + FN_train) * 100\n    specificity_train_lda = TN_train / (TN_train + FP_train) * 100\n    sensitivity_test_lda = TP_test / (TP_test + FN_test) * 100\n    specificity_test_lda = TN_test / (TN_test + FP_test) * 100\n    \n    # Total Predictions\n    total_train = len(y_train)\n    total_test = len(y_test)\n\n    # Summary Table in Required Format\n    summary_lda = pd.DataFrame({\n        \"Data Set\": [\"Training\", \"\", \"Total\", \"Test\", \"\", \"Total\"],\n        \"Observed Classification\": [\"f(vij)obs=0\", \"f(vij)obs=1\", \"\", \"f(vij)obs=0\", \"f(vij)obs=1\", \"\"],\n        \"Stat. Param.\": [\"Sp (%)\", \"Sn (%)\", \"Ac (%)\", \"Sp (%)\", \"Sn (%)\", \"Ac (%)\"],\n        \"Pred. Stats.\": [\n            specificity_train_lda, sensitivity_train_lda, train_accuracy_lda,\n            specificity_test_lda, sensitivity_test_lda, test_accuracy_lda\n        ],\n        \"nj\": [TN_train + FP_train, TP_train + FN_train, total_train, TN_test + FP_test, TP_test + FN_test, total_test],\n        \"f(vij)pred=0\": [TN_train, FN_train, TN_train + FN_train, TN_test, FN_test, TN_test + FN_test],\n        \"f(vij)pred=1\": [FP_train, TP_train, FP_train + TP_train, FP_test, TP_test, FP_test + TP_test]\n    })\n\n    # Print the formatted summary\n    print(\"\\nLDA Performance Summary:\")\n    print(summary_lda.to_string(index=False))\n\nelse:\n    print(\"Multiclass classification detected – Specificity not defined.\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 4. Classification Report for the Training Set (LDA)\nprint(\"Classification Report on Training Set (LDA):\")\nprint(classification_report(y_train, y_train_pred_lda, digits=4))\n\n# 5. Classification Report for the Test Set (LDA)\nprint(\"Classification Report on Test Set (LDA):\")\nprint(classification_report(y_test, y_test_pred_lda, digits=4))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 6. Confusion Matrix for the Training Set (LDA)\nprint(\"Confusion Matrix on Training Set (LDA):\")\nprint(confusion_matrix(y_train, y_train_pred_lda))\n\n# 7. Confusion Matrix for the Test Set (LDA)\nprint(\"Confusion Matrix on Test Set (LDA):\")\nprint(confusion_matrix(y_test, y_test_pred_lda))\n\n# Plot and save confusion matrices for LDA\nplot_confusion_matrix_with_percentages(y_train, y_train_pred_lda, title=\"Training Set (LDA)\", filename=\"LDA_Train_Confusion_Matrix\")\nplot_confusion_matrix_with_percentages(y_test, y_test_pred_lda, title=\"Test Set (LDA)\", filename=\"LDA_Test_Confusion_Matrix\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 8. AUC (Area Under the ROC Curve) for LDA\n# For binary classification (1 and 0):\ny_test_proba_lda = best_lda_model.predict_proba(X_test)[:, 1]  # Probability estimates for the positive class (1)\ntest_auc_lda = roc_auc_score(y_test, y_test_proba_lda)\nprint(f'Test AUC (LDA): {test_auc_lda:.4f}')\n\n# For the training set\ny_train_proba_lda = best_lda_model.predict_proba(X_train)[:, 1]\ntrain_auc_lda = roc_auc_score(y_train, y_train_proba_lda)\nprint(f'Training AUC (LDA): {train_auc_lda:.4f}')\n\n# Plot ROC curve for the Test set\nfpr_test_lda, tpr_test_lda, _ = roc_curve(y_test, y_test_proba_lda)\nroc_auc_test_lda = auc(fpr_test_lda, tpr_test_lda)\n\n# Plot ROC curve for the Training set\nfpr_train_lda, tpr_train_lda, _ = roc_curve(y_train, y_train_proba_lda)\nroc_auc_train_lda = auc(fpr_train_lda, tpr_train_lda)\n\n# Plot the ROC curve\nplt.figure(figsize=(10, 8))\nplt.plot(fpr_test_lda, tpr_test_lda, color='blue', lw=2, label=f'Test ROC curve (AUC = {roc_auc_test_lda:.2f})')\nplt.plot(fpr_train_lda, tpr_train_lda, color='green', lw=2, label=f'Training ROC curve (AUC = {roc_auc_train_lda:.2f})')\nplt.plot([0, 1], [0, 1], color='gray', linestyle='--', lw=2)  # Diagonal line (random classifier)\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('ROC Curve (LDA)')\nplt.legend(loc='lower right')\nplt.grid(True)\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 9. Get feature importances from the best LDA model\nlda_coefficients = best_lda_model.coef_.flatten()\n\n# Create a DataFrame for LDA feature importances\nfeatures_df_lda = pd.DataFrame({\n    'Feature': X_train.columns,\n    'Importance': np.abs(lda_coefficients)\n})\n\n# Sort the DataFrame by importance\nfeatures_df_lda = features_df_lda.sort_values(by='Importance', ascending=False)\n\n# Select the top 20 features for LDA\ntop_20_features_lda = features_df_lda.head(20)\n\n# Plot the top 20 feature importances for LDA\nplot_feature_importance(top_20_features_lda, 'Top 20 Feature Importances (LDA)', '/kaggle/working/Top_20_Feature_Importances_LDA.png')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 2. XGBoost","metadata":{}},{"cell_type":"code","source":"# Initialize the XGBoost model\nxgb_model = xgb.XGBClassifier(use_label_encoder=False, eval_metric='logloss')\n\n# Train the model\nxgb_model.fit(X_train, y_train)\n\n# Predictions\ny_train_pred_xgb = xgb_model.predict(X_train)\ny_test_pred_xgb = xgb_model.predict(X_test)\n\n# Accuracy\ntrain_accuracy_xgb = accuracy_score(y_train, y_train_pred_xgb)\ntest_accuracy_xgb = accuracy_score(y_test, y_test_pred_xgb)\n\nprint(f'XGBoost - Training Accuracy: {train_accuracy_xgb:.2f}')\nprint(f'XGBoost - Test Accuracy: {test_accuracy_xgb:.2f}')\nprint(\"Classification Report for XGBoost (Training Set):\")\nprint(classification_report(y_train, y_train_pred_xgb))\nprint(\"Classification Report for XGBoost (Test Set):\")\nprint(classification_report(y_test, y_test_pred_xgb))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Add Randomized SearchCV**","metadata":{}},{"cell_type":"code","source":"# Define the hyperparameter grid\nparam_distributions_xgb = {\n    'n_estimators': np.arange(50, 201, 10),\n    'max_depth': [3, 6, 10, 15],\n    'learning_rate': np.linspace(0.01, 0.3, 10),\n    'subsample': [0.6, 0.8, 1.0],\n    'colsample_bytree': [0.6, 0.8, 1.0],\n    'gamma': [0, 0.1, 0.2, 0.3],\n    'min_child_weight': [1, 5, 10]\n}\n\n# Set up RandomizedSearchCV\nrandom_search_xgb = RandomizedSearchCV(\n    estimator=xgb_model,\n    param_distributions=param_distributions_xgb,\n    n_iter=30,  \n    cv=3,      \n    verbose=2,\n    random_state=7,\n    n_jobs=-1\n)\n\n# Fit RandomizedSearchCV on the training data\nrandom_search_xgb.fit(X_train, y_train)\n\n# Get the best parameters from RandomizedSearchCV\nbest_params_random_xgb = random_search_xgb.best_params_\nprint(\"Best parameters from Randomized Search for XGBoost:\", best_params_random_xgb)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Add GridSearchCV**","metadata":{}},{"cell_type":"code","source":"# Ensure we don't have invalid n_estimators\nn_estimators_values_xgb = [\n    max(1, best_params_random_xgb['n_estimators'] - 10),\n    best_params_random_xgb['n_estimators'],\n    best_params_random_xgb['n_estimators'] + 10\n]\n\n# Create the parameter grid\nparam_grid_xgb = {\n    'n_estimators': n_estimators_values_xgb,\n    'max_depth': [best_params_random_xgb['max_depth'] - 2, best_params_random_xgb['max_depth'], best_params_random_xgb['max_depth'] + 2],\n    'learning_rate': [best_params_random_xgb['learning_rate'] - 0.05, best_params_random_xgb['learning_rate'], best_params_random_xgb['learning_rate'] + 0.05],\n    'subsample': [best_params_random_xgb['subsample']],\n    'colsample_bytree': [best_params_random_xgb['colsample_bytree']],\n    'gamma': [best_params_random_xgb['gamma']],\n    'min_child_weight': [best_params_random_xgb['min_child_weight']]\n}\n\n# Set up GridSearchCV\ngrid_search_xgb = GridSearchCV(\n    estimator=xgb_model,\n    param_grid=param_grid_xgb,\n    cv=3,\n    verbose=2,\n    n_jobs=-1\n)\n\n# Fit GridSearchCV on the training data\ngrid_search_xgb.fit(X_train, y_train)\n\n# Get the best parameters from GridSearchCV\nbest_params_grid_xgb = grid_search_xgb.best_params_\nprint(\"Best parameters from Grid Search for XGBoost:\", best_params_grid_xgb)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Train final XGBoost model with the best hyperparameters\nbest_xgb_model = xgb.XGBClassifier(**best_params_grid_xgb, use_label_encoder=False, eval_metric='logloss', random_state=42)\n\n# Perform K-Fold Cross-Validation on the final tuned model\nfinal_cv_scores = cross_val_score(best_xgb_model, X, y, cv=kf, scoring='accuracy')\n\nprint(f'Final XGBoost Model - Mean Cross-Validation Accuracy: {final_cv_scores.mean():.4f} ± {final_cv_scores.std():.4f}')\n\n# Train on full dataset and evaluate predictions\nbest_xgb_model.fit(X, y)\n\ny_pred = best_xgb_model.predict(X)\n\n# Predict using the best model\nbest_xgb_model = grid_search_xgb.best_estimator_\ny_test_pred_xgb = best_xgb_model.predict(X_test)\n\n# Evaluate the model performance\ntest_accuracy_xgb = accuracy_score(y_test, y_test_pred_xgb)\nprint(f'Test Set Accuracy with Best Hyperparameters from Grid Search: {test_accuracy_xgb * 100:.2f}%')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Evaluate the model**","metadata":{}},{"cell_type":"code","source":"# Get the best model from Grid Search\nbest_xgb_model = grid_search_xgb.best_estimator_\n\n# Predictions on the training set\ny_train_pred_xgb = best_xgb_model.predict(X_train)\n\n# Predictions on the test set\ny_test_pred_xgb = best_xgb_model.predict(X_test)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 1. Accuracy Score for the Training Set\ntrain_accuracy_xgb = accuracy_score(y_train, y_train_pred_xgb)\nprint(f'Train Set Accuracy: {train_accuracy_xgb * 100:.2f}%')\n\n# 2. Accuracy Score for the Test Set\ntest_accuracy_xgb = accuracy_score(y_test, y_test_pred_xgb)\nprint(f'Test Set Accuracy: {test_accuracy_xgb * 100:.2f}%')\n\n# How many predictions were correct out of the total for both sets\ntrain_correct_xgb = sum(y_train_pred_xgb == y_train)\ntest_correct_xgb = sum(y_test_pred_xgb == y_test)\n\nprint(f'Training set: {train_correct_xgb} correct out of {len(y_train)}')\nprint(f'Test set: {test_correct_xgb} correct out of {len(y_test)}')\n\n# Sensitivity (Recall) and Specificity for Training Set\ncm_train_xgb = confusion_matrix(y_train, y_train_pred_xgb)\nif cm_train_xgb.shape == (2, 2):\n    TN, FP, FN, TP = cm_train_xgb.ravel()\n    sensitivity_train_xgb = TP / (TP + FN)\n    specificity_train_xgb = TN / (TN + FP)\nelse:\n    sensitivity_train_xgb = recall_score(y_train, y_train_pred_xgb, average='macro')\n    specificity_train_xgb = np.nan  # Not defined for multiclass\n\n# Sensitivity (Recall) and Specificity for Test Set\ncm_test_xgb = confusion_matrix(y_test, y_test_pred_xgb)\nif cm_test_xgb.shape == (2, 2):\n    TN, FP, FN, TP = cm_test_xgb.ravel()\n    sensitivity_test_xgb = TP / (TP + FN)\n    specificity_test_xgb = TN / (TN + FP)\nelse:\n    sensitivity_test_xgb = recall_score(y_test, y_test_pred_xgb, average='macro')\n    specificity_test_xgb = np.nan  # Not defined for multiclass\n\n# 3. Summary Table\nsummary_xgb = pd.DataFrame({\n    'Metric': ['Accuracy', 'Sensitivity', 'Specificity'],\n    'Training Set': [train_accuracy_xgb, sensitivity_train_xgb, specificity_train_xgb],\n    'Test Set': [test_accuracy_xgb, sensitivity_test_xgb, specificity_test_xgb]\n})\n\nprint(\"\\nXGBoost Performance Summary:\")\nprint(summary_xgb)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Initialize variables\nif cm_train_xgb.shape == (2, 2):  # Binary classification\n    TN_train, FP_train, FN_train, TP_train = cm_train_xgb.ravel()\n    TN_test, FP_test, FN_test, TP_test = cm_test_xgb.ravel()\n    \n    # Sensitivity (Recall) & Specificity calculations\n    sensitivity_train_xgb = TP_train / (TP_train + FN_train) * 100\n    specificity_train_xgb = TN_train / (TN_train + FP_train) * 100\n    sensitivity_test_xgb = TP_test / (TP_test + FN_test) * 100\n    specificity_test_xgb = TN_test / (TN_test + FP_test) * 100\n    \n    # Total Predictions\n    total_train = len(y_train)\n    total_test = len(y_test)\n\n    # Summary Table in Required Format\n    summary_xgb = pd.DataFrame({\n        \"Data Set\": [\"Training\", \"\", \"Total\", \"Test\", \"\", \"Total\"],\n        \"Observed Classification\": [\"f(vij)obs=0\", \"f(vij)obs=1\", \"\", \"f(vij)obs=0\", \"f(vij)obs=1\", \"\"],\n        \"Stat. Param.\": [\"Sp (%)\", \"Sn (%)\", \"Ac (%)\", \"Sp (%)\", \"Sn (%)\", \"Ac (%)\"],\n        \"Pred. Stats.\": [\n            specificity_train_xgb, sensitivity_train_xgb, train_accuracy_xgb,\n            specificity_test_xgb, sensitivity_test_xgb, test_accuracy_xgb\n        ],\n        \"nj\": [TN_train + FP_train, TP_train + FN_train, total_train, TN_test + FP_test, TP_test + FN_test, total_test],\n        \"f(vij)pred=0\": [TN_train, FN_train, TN_train + FN_train, TN_test, FN_test, TN_test + FN_test],\n        \"f(vij)pred=1\": [FP_train, TP_train, FP_train + TP_train, FP_test, TP_test, FP_test + TP_test]\n    })\n\n    # Print the formatted summary\n    print(\"\\nXGBoost Performance Summary:\")\n    print(summary_xgb.to_string(index=False))\n\nelse:\n    print(\"Multiclass classification detected – Specificity not defined.\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 4. Classification Report for the Training Set\nprint(\"Classification Report on Training Set:\")\nprint(classification_report(y_train, y_train_pred_xgb, digits=4))\n\n# 5. Classification Report for the Test Set\nprint(\"Classification Report on Test Set:\")\nprint(classification_report(y_test, y_test_pred_xgb, digits=4))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 6. Confusion Matrix for the Training Set\nprint(\"Confusion Matrix on Training Set:\")\nprint(confusion_matrix(y_train, y_train_pred_xgb))\n\n# 7. Confusion Matrix for the Test Set\nprint(\"Confusion Matrix on Test Set:\")\nprint(confusion_matrix(y_test, y_test_pred_xgb))\n\n# Plot and save confusion matrices for XGB\nplot_confusion_matrix_with_percentages(y_train, y_train_pred_xgb, title=\"Training Set (XGB)\", filename=\"XGB_Train_Confusion_Matrix\")\nplot_confusion_matrix_with_percentages(y_test, y_test_pred_xgb, title=\"Test Set (XGB)\", filename=\"XGB_Test_Confusion_Matrix\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 8. AUC (Area Under the ROC Curve) for XGBoost\n# For binary classification (1 and 0):\ny_test_proba_xgb = best_xgb_model.predict_proba(X_test)[:, 1]  # Probability estimates for the positive class (1)\ntest_auc_xgb = roc_auc_score(y_test, y_test_proba_xgb)\nprint(f'Test AUC (XGBoost): {test_auc_xgb:.4f}')\n\n# For the training set\ny_train_proba_xgb = best_xgb_model.predict_proba(X_train)[:, 1]\ntrain_auc_xgb = roc_auc_score(y_train, y_train_proba_xgb)\nprint(f'Training AUC (XGBoost): {train_auc_xgb:.4f}')\n\n# Plot ROC curve for the Test set\nfpr_test_xgb, tpr_test_xgb, _ = roc_curve(y_test, y_test_proba_xgb)\nroc_auc_test_xgb = auc(fpr_test_xgb, tpr_test_xgb)\n\n# Plot ROC curve for the Training set\nfpr_train_xgb, tpr_train_xgb, _ = roc_curve(y_train, y_train_proba_xgb)\nroc_auc_train_xgb = auc(fpr_train_xgb, tpr_train_xgb)\n\n# Plot the ROC curve\nplt.figure(figsize=(10, 8))\nplt.plot(fpr_test_xgb, tpr_test_xgb, color='blue', lw=2, label=f'Test ROC curve (AUC = {roc_auc_test_xgb:.2f})')\nplt.plot(fpr_train_xgb, tpr_train_xgb, color='green', lw=2, label=f'Training ROC curve (AUC = {roc_auc_train_xgb:.2f})')\nplt.plot([0, 1], [0, 1], color='gray', linestyle='--', lw=2)  # Diagonal line (random classifier)\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('ROC Curve (XGBoost)')\nplt.legend(loc='lower right')\nplt.grid(True)\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 9. Get feature importances from the best XGB model\nfeature_importances_xgb = best_xgb_model.feature_importances_\n\n# Create a DataFrame for XGB feature importances\nfeatures_df_xgb = pd.DataFrame({\n    'Feature': X_train.columns,\n    'Importance': feature_importances_xgb\n})\n\n# Sort the DataFrame by importance\nfeatures_df_xgb = features_df_xgb.sort_values(by='Importance', ascending=False)\n\n# Select the top 20 features for XGB (or adjust the number as needed)\ntop_20_features_xgb = features_df_xgb.head(20)\n\n# Plot the top 20 feature importances for XGB\nplot_feature_importance(top_20_features_xgb, 'Top 20 Feature Importances (XGB)', '/kaggle/working/Top_20_Feature_Importances_XGB.png')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Make Predictions**","metadata":{}},{"cell_type":"code","source":"# Cargar el archivo Excel\nfile_path = '/kaggle/input/predictions-python/XGB pred.xlsx'\nnew_data = pd.read_excel(file_path)\n\n# Mostrar las primeras filas para verificar que se cargó correctamente\nprint(new_data.head())","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Obtener las columnas que el modelo espera\nmodel_columns = best_xgb_model.get_booster().feature_names\n\n# Filtrar el DataFrame para que solo tenga las columnas necesarias\nnew_data_filtered = new_data[model_columns if all(col in new_data.columns for col in model_columns) else new_data.columns.intersection(model_columns)]\n\n# Verificar las columnas seleccionadas\nprint(\"Columnas seleccionadas:\", new_data_filtered.columns.tolist())","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\n\n# Identificar las columnas faltantes\nmissing_columns = [col for col in model_columns if col not in new_data_filtered.columns]\n\n# Añadir las columnas faltantes con NaN\nfor col in missing_columns:\n    new_data_filtered[col] = np.nan\n\n# Asegurarse de que el orden de las columnas coincida exactamente con el modelo\nnew_data_filtered = new_data_filtered[model_columns]\n\n# Verificar que todas las columnas ahora están presentes\nprint(\"Columnas después de añadir las faltantes:\", new_data_filtered.columns.tolist())","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Realizar las predicciones\npredictions = best_xgb_model.predict(new_data_filtered)\n\n# Mostrar las predicciones\nprint(\"Predicciones realizadas:\", predictions)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Añadir las predicciones al DataFrame original\nnew_data['Predicciones'] = predictions\n\n# Guardar el DataFrame con las predicciones en un nuevo archivo\nnew_data.to_excel('/kaggle/working/Predicciones_Resultados.xlsx', index=False)\n\nprint(\"Predicciones guardadas en 'Predicciones_Resultados.xlsx'\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Evaluate The Predictions**","metadata":{}},{"cell_type":"code","source":"# Set the plot style\nsns.set(style=\"whitegrid\")\n\n# Create a bar chart\nplt.figure(figsize=(8, 6)) \nsns.barplot(x='Deriv', y='Pred', data=results_df, palette='viridis')\n\n# Customize the plot with increased font sizes\nplt.title('Comparison of Success Probability in Assays', fontsize=18)\nplt.xlabel('Molecules and Derivatives', fontsize=14)\nplt.ylabel('Success Probability (Pred)', fontsize=14)\nplt.ylim(0, 1)  \nplt.xticks(rotation=45, fontsize=14) \nplt.yticks(fontsize=14)\n\n# Save the plot\nplt.tight_layout()  # Ensure everything fits without overlapping\nplt.savefig('/kaggle/working/Success_Probability_Assays.png', dpi=300)  # Save the figure\n\n# Show the plot\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Group by assay type and calculate the average success\nassay_success = results_df.groupby('c10=Assay Type')['Pred'].mean().reset_index()\n\n# Create a dictionary to map the assay type letter to its corresponding name\nassay_type_map = {\n    'B': 'Binding',\n    'F': 'Functional',\n    'A': 'ADME',\n    'P': 'Physicochemical'\n}\n\n# Map the assay type column to the full names\nassay_success['Assay Type'] = assay_success['c10=Assay Type'].map(assay_type_map)\n\n# Set the plot style\nsns.set(style=\"whitegrid\")\n\n# Create the bar plot with the 'viridis' palette\nplt.figure(figsize=(6, 6))\nsns.barplot(x='Assay Type', y='Pred', data=assay_success, palette='viridis')\n\n# Customize the plot with increased font sizes\nplt.title('Success by Assay Type', fontsize=18)\nplt.xlabel('Assay Type', fontsize=14)\nplt.ylabel('Success Average', fontsize=14)\nplt.xticks(rotation=45, fontsize=14)\nplt.yticks(fontsize=14)\n\n# Save the image\nsave_path = '/kaggle/working/success_by_assay_type.png'\nplt.tight_layout()  # Adjust layout to avoid overlap\nplt.savefig(save_path, dpi=300, bbox_inches='tight')\nprint(f'Image saved at: {save_path}')\n\n# Show the plot\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# List of proteins related to Calmodulin (your specified proteins)\ncalmodulin_related = [\n    \"Calmodulin\",\n    \"CaM kinase I alpha\",\n    \"CaM kinase I delta\",\n    \"CaM kinase II\",\n    \"CaM kinase II beta\",\n    \"CaM kinase II delta\",\n    \"CaM kinase II gamma\",\n    \"CaM kinase IV\",\n    \"CaM-kinase kinase alpha\",\n    \"CaM-kinase kinase beta\",cmap = sns.diverging_palette(150, 10, s=90, l=50, as_cmap=True)\n\n    \"Calcium/calmodulin-dependent protein kinase type 1B\"\n]\n\n# Step 1: Filter the dataframe based on 'Target Organism' being 'Homo sapiens' \n# and 'Target Name' being one of the Calmodulin-related proteins, plus 'Assay Type' = B\nfiltered_df = results_df[\n    (results_df['c04=Target Organism'] == 'Homo sapiens') & \n    (results_df['c01=Target Name'].isin(calmodulin_related)) & \n    (results_df['c10=Assay Type'] == 'B') \n]\n\n# Step 2: Focus on derivatives 1a-2c and Riluzole\nderivatives = ['1a', '1b', '1c', '1d', '2a', '2b', '2c']\nriluzole_data = results_df[results_df['Deriv'] == 'Riluzole']\n\n# Filter for the derivatives 1a-2c in the filtered dataframe\nfiltered_derivatives = filtered_df[filtered_df['Deriv'].isin(derivatives)]\n\n# Step 3: Create a new column to calculate the relative performance compared to Riluzole\n# We will focus on the 'Pred' column (1 or 0 for success/failure)\nriluzole_preds = riluzole_data[riluzole_data['c01=Target Name'].isin(calmodulin_related)]\n\n# Create a mapping for Riluzole predictions by Target Name\nriluzole_pred_map = riluzole_preds.set_index('c01=Target Name')['Pred'].to_dict()\n\n# Calculate the relative performance for derivatives vs Riluzole\nfiltered_derivatives['Relative Performance'] = filtered_derivatives.apply(\n    lambda row: row['Pred'] - riluzole_pred_map.get(row['c01=Target Name'], 0), axis=1\n)\n\n# Step 4: Pivot the dataframe to create the heatmap data with Deriv as columns and Target Name as rows\nheatmap_data = filtered_derivatives.pivot_table(\n    index='c01=Target Name', columns='Deriv', values='Relative Performance', aggfunc='mean'\n)\n\n# Reorder the rows to match your specified protein list\nheatmap_data = heatmap_data.reindex(calmodulin_related)\n\n# Step 5: Normalize the data for color gradient from red (worse) to green (better)\nmin_val, max_val = heatmap_data.min().min(), heatmap_data.max().max()\nnormalized_data = (heatmap_data - min_val) / (max_val - min_val) * 2 - 1  # Scaling to [-1, 1]\n\n# Step 6: Plot the heatmap with gradient colors\nplt.figure(figsize=(12, 8))\ncmap = sns.diverging_palette(0, 120, as_cmap=True)\n\nsns.heatmap(normalized_data, annot=True, cmap=cmap, fmt='.2f', cbar_kws={'label': 'Relative Performance (vs Riluzole)'})\n\n# Add title and labels\nplt.title('Comparison of Riluzole Derivatives vs Calmodulin-Related Proteins (Binding Assays)', fontsize=16)\nplt.xlabel('Riluzole Derivatives', fontsize=14)\nplt.ylabel('Calmodulin-Related Proteins', fontsize=14)\n\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# List of proteins related to Calmodulin (with Calmodulin first and sorted)\ncalmodulin_related = [\n    \"Calmodulin\",\n    \"CaM kinase I alpha\",\n    \"CaM kinase I delta\",\n    \"CaM kinase II\",\n    \"CaM kinase II beta\",\n    \"CaM kinase II delta\",\n    \"CaM kinase II gamma\",\n    \"CaM kinase IV\",\n    \"CaM-kinase kinase alpha\",\n    \"CaM-kinase kinase beta\",\n    \"Calcium/calmodulin-dependent protein kinase type 1B\"\n]\n\n# Step 1: Filter the dataframe for Homo sapiens, calmodulin-related proteins, and binding assays (B)\nfiltered_df = results_df[\n    (results_df['c04=Target Organism'] == 'Homo sapiens') &\n    (results_df['c01=Target Name'].isin(calmodulin_related)) &\n    (results_df['c10=Assay Type'] == 'B')\n]\n\n# Step 2: Focus on derivatives 1a-2c and Riluzole\nderivatives = ['1a', '1b', '1c', '1d', '2a', '2b', '2c']\nriluzole_data = results_df[results_df['Deriv'] == 'Riluzole']\n\n# Filter for derivatives in the filtered dataframe\nfiltered_derivatives = filtered_df[filtered_df['Deriv'].isin(derivatives)]\n\n# Step 3: Create a new column to calculate relative performance vs Riluzole\nriluzole_preds = riluzole_data[riluzole_data['c01=Target Name'].isin(calmodulin_related)]\nriluzole_pred_map = riluzole_preds.set_index('c01=Target Name')['Pred'].to_dict()\n\n# Calculate relative performance\nfiltered_derivatives['Relative Performance'] = filtered_derivatives.apply(\n    lambda row: row['Pred'] - riluzole_pred_map.get(row['c01=Target Name'], 0), axis=1\n)\n\n# Step 4: Pivot to create the heatmap data\nheatmap_data = filtered_derivatives.pivot_table(\n    index='c01=Target Name', columns='Deriv', values='Relative Performance', aggfunc='mean'\n)\n\n# Step 5: Filter out proteins without any data (remove NaN rows)\nheatmap_data = heatmap_data.dropna(how='all')\n\n# Reorder the proteins to match the specified order\nheatmap_data = heatmap_data.reindex(calmodulin_related)\n\n# Step 6: Normalize for color gradient from red (worse) to green (better)\nmin_val, max_val = heatmap_data.min().min(), heatmap_data.max().max()\nnormalized_data = (heatmap_data - min_val) / (max_val - min_val) * 2 - 1  # Scale to [-1, 1]\n\n# Step 7: Plot the heatmap with red-green gradient\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nplt.figure(figsize=(12, 6))\ncmap = sns.diverging_palette(15, 150, as_cmap=True)  # Green to red\n\nsns.heatmap(normalized_data, annot=True, cmap=cmap, fmt='.2f', \n            cbar_kws={'label': 'Relative Performance (vs Riluzole)'})\n\n# Add titles and labels with extra padding for the title\nplt.title('Comparison of Riluzole Derivatives vs Calmodulin-Related Proteins (Binding Assays)', \n          fontsize=18, pad=15)  # Added pad=20 to separate the title\n\nplt.xlabel('Riluzole Derivatives', fontsize=14)\nplt.ylabel('Calmodulin-Related Proteins', fontsize=14)\n\n# Save the plot as a PNG image\nsave_path = '/kaggle/working/heatmap_riluzole_derivatives_vs_calmodulin.png'\nplt.tight_layout()  # Adjust layout to prevent clipping\nplt.savefig(save_path, dpi=300, bbox_inches='tight')  # Save the image\n\nprint(f\"Image saved at: {save_path}\")  # Notify the user\n\n# Show the plot\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# MODEL COMPARISON","metadata":{}},{"cell_type":"code","source":"# Initialize model results dictionary\nmodel_results = {}\n\n# Define the models\nmodels = {\n    'Random Forest': RandomForestClassifier(),\n    'Support Vector Machine': SVC(probability=True),\n    'Decision Tree': DecisionTreeClassifier(),\n    'K-Nearest Neighbors': KNeighborsClassifier(),\n    'Gradient Boosting': GradientBoostingClassifier(),\n    'XGBoost': xgb.XGBClassifier(use_label_encoder=False, eval_metric='logloss')\n}\n\n# Train and evaluate each model\nfor model_name, model in models.items():\n    print(f'Training {model_name}...')\n    \n    # Train the model\n    model.fit(X_train, y_train)\n    \n    # Predictions\n    y_train_pred = model.predict(X_train)\n    y_test_pred = model.predict(X_test)\n    \n    # Accuracy\n    train_accuracy = accuracy_score(y_train, y_train_pred)\n    test_accuracy = accuracy_score(y_test, y_test_pred)\n    \n    # AUROC for Test Data\n    y_test_proba = model.predict_proba(X_test)[:, 1]  \n    test_roc_auc = roc_auc_score(y_test, y_test_proba)\n    \n    # AUROC for Train Data\n    y_train_proba = model.predict_proba(X_train)[:, 1] \n    train_roc_auc = roc_auc_score(y_train, y_train_proba)\n    \n    # Save results\n    model_results[model_name] = {\n        'Train Accuracy': train_accuracy,\n        'Test Accuracy': test_accuracy,\n        'Train ROC AUC': train_roc_auc,\n        'Test ROC AUC': test_roc_auc\n    }\n\n    print(f'{model_name} - Train Accuracy: {train_accuracy:.2f}, Test Accuracy: {test_accuracy:.2f}, Train ROC AUC: {train_roc_auc:.2f}, Test ROC AUC: {test_roc_auc:.2f}')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Initialize model results dictionary\nmodel_results = {}\n\n# Define the models\nmodels = {\n    'Random Forest': RandomForestClassifier(),\n    'Support Vector Machine': SVC(probability=True),\n    'Decision Tree': DecisionTreeClassifier(),\n    'K-Nearest Neighbors': KNeighborsClassifier(),\n    'Gradient Boosting': GradientBoostingClassifier(),\n    'XGBoost': xgb.XGBClassifier(use_label_encoder=False, eval_metric='logloss')\n}\n\n# Train and evaluate each model\nfor model_name, model in models.items():\n    print(f'Training {model_name}...')\n    \n    # Train the model\n    model.fit(X_train, y_train)\n    \n    # Predictions\n    y_train_pred = model.predict(X_train)\n    y_test_pred = model.predict(X_test)\n    \n    # Accuracy\n    train_accuracy = accuracy_score(y_train, y_train_pred)\n    test_accuracy = accuracy_score(y_test, y_test_pred)\n    \n    # AUROC for Test Data\n    y_test_proba = model.predict_proba(X_test)[:, 1]  \n    test_roc_auc = roc_auc_score(y_test, y_test_proba)\n    \n    # AUROC for Train Data\n    y_train_proba = model.predict_proba(X_train)[:, 1] \n    train_roc_auc = roc_auc_score(y_train, y_train_proba)\n    \n    # Save results\n    model_results[model_name] = {\n        'Train Accuracy': train_accuracy,\n        'Test Accuracy': test_accuracy,\n        'Train ROC AUC': train_roc_auc,\n        'Test ROC AUC': test_roc_auc\n    }\n\n    print(f'{model_name} - Train Accuracy: {train_accuracy:.2f}, Test Accuracy: {test_accuracy:.2f}, Train ROC AUC: {train_roc_auc:.2f}, Test ROC AUC: {test_roc_auc:.2f}')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Create a figure\nplt.figure(figsize=(8, 6))\n\n# Plot ROC curves for each model\nfor model_name, model in models.items():\n    # Predict probabilities\n    y_test_proba = model.predict_proba(X_test)[:, 1]\n    \n    # ROC curve\n    fpr, tpr, _ = roc_curve(y_test, y_test_proba)\n    \n    # Plot ROC curve\n    plt.plot(fpr, tpr, label=f'{model_name} (AUC = {model_results[model_name][\"Test ROC AUC\"]:.2f})')\n\n# Add diagonal reference line\nplt.plot([0, 1], [0, 1], 'k--', lw=2)\n\n# Add labels, title, and legend\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate', fontsize=12)\nplt.ylabel('True Positive Rate', fontsize=12)\nplt.title('ROC Curves Comparison (Test)', fontsize=14)\nplt.legend(loc='lower right')\n\n# Save the figure\nsave_path = '/kaggle/working/roc_curves_comparison.png'\nplt.savefig(save_path, dpi=300)\n\n# Show the figure\nplt.show()\n\nprint(f\"ROC curve figure saved at: {save_path}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Create a figure\nplt.figure(figsize=(8, 6))\n\n# Plot ROC curves for each model using training data\nfor model_name, model in models.items():\n    # Predict probabilities\n    y_train_proba = model.predict_proba(X_train)[:, 1]\n    \n    # ROC curve\n    fpr, tpr, _ = roc_curve(y_train, y_train_proba)\n    \n    # Plot ROC curve\n    plt.plot(fpr, tpr, label=f'{model_name} (AUC = {model_results[model_name][\"Train ROC AUC\"]:.2f})')\n\n# Add diagonal reference line\nplt.plot([0, 1], [0, 1], 'k--', lw=2)\n\n# Add labels, title, and legend\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate (FPR)', fontsize=12)\nplt.ylabel('True Positive Rate (TPR)', fontsize=12)\nplt.title('ROC Curves Comparison (Training)', fontsize=14)\nplt.legend(loc='lower right')\n\n# Save the figure\nsave_path = '/kaggle/working/roc_curves_train_comparison.png'\nplt.savefig(save_path, dpi=300) \n\n# Show the figure\nplt.show()\n\nprint(f\"ROC curve figure saved at: {save_path}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Create DataFrame from model_results\nresults_df = pd.DataFrame([\n    {\n        'Model': model_name,\n        'Train Accuracy': metrics['Train Accuracy'],\n        'Test Accuracy': metrics['Test Accuracy'],\n        'Train ROC AUC': metrics['Train ROC AUC'],  # Include Train ROC AUC\n        'Test ROC AUC': metrics['Test ROC AUC'],    # Include Test ROC AUC\n    }\n    for model_name, metrics in model_results.items()\n])\n\n# Print the final DataFrame with the desired columns\nprint(results_df[['Model', 'Train Accuracy', 'Test Accuracy', 'Train ROC AUC', 'Test ROC AUC']].to_string(index=False))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Initialize model results dictionary\nmodel_results = {}\n\n# Define the models\nmodels = {\n    'Random Forest': RandomForestClassifier(),\n    'Support Vector Machine': SVC(probability=True),\n    'Decision Tree': DecisionTreeClassifier(),\n    'K-Nearest Neighbors': KNeighborsClassifier(),\n    'Gradient Boosting': GradientBoostingClassifier(),\n    'XGBoost': xgb.XGBClassifier(use_label_encoder=False, eval_metric='logloss'),\n    'LDA': LinearDiscriminantAnalysis()\n}\n\n# Train and evaluate each model using 10-fold cross-validation\nfor model_name, model in models.items():\n    print(f'Evaluating {model_name} with 10-fold cross validation...')\n    \n    # Compute cross-validation scores\n    train_cv_accuracy = cross_val_score(model, X_train, y_train, cv=10, scoring='accuracy').mean()\n    test_cv_accuracy = cross_val_score(model, X_test, y_test, cv=10, scoring='accuracy').mean()\n    \n    train_cv_roc_auc = cross_val_score(model, X_train, y_train, cv=10, scoring='roc_auc').mean()\n    test_cv_roc_auc = cross_val_score(model, X_test, y_test, cv=10, scoring='roc_auc').mean()\n    \n    # Save results\n    model_results[model_name] = {\n        'Train CV Accuracy': train_cv_accuracy,\n        'Test CV Accuracy': test_cv_accuracy,\n        'Train CV ROC AUC': train_cv_roc_auc,\n        'Test CV ROC AUC': test_cv_roc_auc\n    }\n\n    print(f'{model_name} - Train CV Accuracy: {train_cv_accuracy:.2f}, Test CV Accuracy: {test_cv_accuracy:.2f}, Train CV ROC AUC: {train_cv_roc_auc:.2f}, Test CV ROC AUC: {test_cv_roc_auc:.2f}')\n\n# Create DataFrame from model_results\nresults_df = pd.DataFrame([\n    {\n        'Model': model_name,\n        'Train CV Accuracy': metrics['Train CV Accuracy'],\n        'Test CV Accuracy': metrics['Test CV Accuracy'],\n        'Train CV ROC AUC': metrics['Train CV ROC AUC'],\n        'Test CV ROC AUC': metrics['Test CV ROC AUC'],\n    }\n    for model_name, metrics in model_results.items()\n])\n\n# Print the final DataFrame with the desired columns\nprint(results_df[['Model', 'Train CV Accuracy', 'Test CV Accuracy', 'Train CV ROC AUC', 'Test CV ROC AUC']].to_string(index=False))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}